{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "862dd177",
   "metadata": {},
   "source": [
    "# Import all the packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54116646",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from methods_overlapping import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db0df580",
   "metadata": {},
   "outputs": [],
   "source": [
    "temps=[1994,1995,1996, 1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019]\n",
    "l_gsa=[1, 2, 5, 6, 7, 8, 9, 10, 11, 15, 16, 17, 18, 19, 20, 22, 23, 25]\n",
    "deltaT=2019-1994 \n",
    "l_species=['ACATPAL','ALEPROS','ANTHANT','APHIMIN','ARGESPY','ARGRHEM','ARNOIMP','ARNOLAT','ARNORUP','ARNOTHO','ASPICUC','ASPIOBS','AULOFIL','BASOPRO','BATHDUB','BELLAPO','BENTROB','BLENOCE','BOTHPOD','BUGLLUT','CALMLYR','CALMMAC','CALMPHA','CALMRIS','CAPOAPE','CARPACU','CATAALL','CECACIR','CENONIG','CENTGRA','CENTUYA','CEPOMAC','CHIMMON','CITHMAC','CLORAGA','COELCOE','DASIPAS','DENTDEN','DICOHEX','DIPLANN','DIPLVUL','ECHIDEN','EPIGCON','EPIGDEN','EPIGTEL','ETMOSPI','EUTRGUR','GADAMAR','GADIARG','GAIDBIS','GAIDMED','GALEGAL','GALUATL','GALUMEL','GLOSLEI','GNATMYS','GOBIFRI','GOBINIG','GOBIQUA','GOBISAN','GOBISUE','GONODEN','GYMNALT','HELIDAC','HEPTPER','HEXAGRI','HOPLMED','HYMEITA','LEPICAU','LEPMBOS','LEPMWHS','LEPTCAV','LEPTDIE','LOPHSPP','MACOSCO','MAURMUE','MERLMER','MICMPOU','MICUOCE','MICUVAR','MOLVDYP','MOLVMOL','MONOHIS','MORAMOR','MULLBAR','MUSTAST','MUSTMUS','MYCOPUN','NETOBRE','NETTMEL','NEZUSCL','OXYNCEN','PAGEACA','PAGEBOG','PAGEERY','PARALEP','PERICAT','PHYIBLE','PHYSDAL','PLATFLE','POLYAME','POMSMAR','PSETMAX','RAJAALB','RAJAAST','RAJABRA','RAJACIR','RAJACLA','RAJAFUL','RAJAMEL','RAJAMIR','RAJAMON','RAJANAE','RAJAOXY','RAJAPOL','SCHEOVA','SCOHRHO','SCORELO','SCORLOP','SCORNOT','SCORPOR','SCORSCO','SCYMLIC','SCYOCAN','SCYOSTE','SERACAB','SERAHEP','SOLEIMP','SOLEKLE','SOLELAS','SOLEVUL','SPARPAG','SPODCAN','SQUAACA','SQUABLA','SQUTACU','SQUTSQU','SYMBVER','SYMPLIG','SYMPNIG','SYNDSAU','TORPMAR','TORPNOB','TORPTOR','TRAHARA','TRAHDRA','TRAHRAD','TRARTRA','TRIGLUC','TRIGLYR','TRIPLAS','TRISCAP','URANSCA','ZEUSFAB']\n",
    "years=temps\n",
    "q_MEDITS=154"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67ed2b",
   "metadata": {},
   "source": [
    "# Import the Functinal/phylogenetic and taxonomic matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3002d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_fonction=['ACATPAL','ALEPROS','ANTHANT','APHIMIN','ARGESPP','ARGESPY','ARGRHEM','ARNOIMP','ARNOLAT','ARNORUP','ARNOTHO','ASPICUC','ASPIOBS','AULOFIL','BASOPRO','BATHDUB','BELLAPO','BENSGLA','BENTROB','BLENOCE','BOOPBOO','BOTHPOD','BUGLLUT','CALMLYR','CALMMAC','CALMPHA','CALMRIS','CAPOAPE','CARPACU','CATAALL','CECACIR','CENONIG','CENTGRA','CENTUYA','CEPOMAC','CERAMAD','CHAUSLO','CHIMMON','CITHMAC','CLORAGA','COELCOE','DASIPAS','DENTDEN','DIAPHOL','DIAPRAF','DICOHEX','DIPLANN','DIPLVUL','ECHIBRU','ECHIDEN','ENGRENC','EPIGCON','EPIGDEN','EPIGTEL','EPINAEN','ETMOSPI','EUTRGUR','GADAMAR','GADIARG','GAIDBIS','GAIDMED','GALEGAL','GALUATL','GALUMEL','GLOSLEI','GNATMYS','GOBIFRI','GOBINIG','GOBIQUA','GOBISAN','GOBISUE','GONODEN','GYMNALT','HELIDAC','HEPTPER','HEXAGRI','HOPLMED','HYGOBEN','HYGOHIG','HYMEITA','LABRVIR','LAMACRO','LEPICAU','LEPMBOS','LEPMWHS','LEPTCAV','LEPTDIE','LITHMOR','LOBIDOF','LOPHSPP','MACOSCO','MAURMUE','MERLMER','MICMPOU','MICUOCE','MICUVAR','MOLVDYP','MOLVMOL','MONOHIS','MORAMOR','MULLBAR','MUSTAST','MUSTMUS','MYCOPUN','NETOBRE','NETTMEL','NEZUSCL','NOTSBOL','NOTSELO','OXYNCEN','PAGEACA','PAGEBEL','PAGEBOG','PAGEERY','PARALEP','PARLCOR','PERICAT','PHYIBLE','PHYSDAL','PLATFLE','POLYAME','POMSMAR','PSETMAX','RAJAALB','RAJAAST','RAJABRA','RAJACIR','RAJACLA','RAJAFUL','RAJAMEL','RAJAMIR','RAJAMON','RAJANAE','RAJAOXY','RAJAPOL','RAJAUND','SCHEOVA','SCOHRHO','SCORELO','SCORLOP','SCORNOT','SCORPOR','SCORSCO','SCYMLIC','SCYOCAN','SCYOSTE','SERACAB','SERAHEP','SOLEIMP','SOLEKLE','SOLELAS','SOLESEN','SOLEVUL','SPARPAG','SPICMAE','SPICSMA','SPODCAN','SQUAACA','SQUABLA','SQUTACU','SQUTOCL','SQUTSQU','STOMBOA','SYMBVER','SYMPLIG','SYMPNIG','SYNDSAU','TORPMAR','TORPNOB','TORPTOR','TRACMED','TRACPIC','TRACTRA','TRAHARA','TRAHDRA','TRAHRAD','TRARTRA','TRIGLUC','TRIGLYR','TRIPLAS','TRISCAP','UPENPOR','URANSCA','VINCPOW','ZENOCON','ZEUSFAB']\n",
    "indices_fonction=dict()\n",
    "c=0\n",
    "for i in l_fonction:\n",
    "    indices_fonction[i]=c\n",
    "    c=c+1\n",
    "#print(indices_fonction)\n",
    "#print(len(l))\n",
    "matrice_fonction = np.loadtxt(\"matrice_fonction_esp.txt\")\n",
    "#print(matrice_fonction)\n",
    "\n",
    "\n",
    "\n",
    "l_taxonomy=['ACATPAL','ALEPROS','ANTHANT','APHIMIN','ARGESPP','ARGESPY','ARGRHEM','ARNOIMP','ARNOLAT','ARNORUP','ARNOTHO','ASPICUC','ASPIOBS','AULOFIL','BASOPRO','BATHDUB','BELLAPO','BENSGLA','BENTROB','BLENOCE','BOOPBOO','BOTHPOD','BUGLLUT','CALMLYR','CALMMAC','CALMPHA','CALMRIS','CAPOAPE','CARPACU','CATAALL','CECACIR','CENONIG','CENTGRA','CENTUYA','CEPOMAC','CERAMAD','CHAUSLO','CHIMMON','CITHMAC','CLORAGA','COELCOE','DASIPAS','DENTDEN','DIAPHOL','DIAPRAF','DICOHEX','DIPLANN','DIPLVUL','ECHIBRU','ECHIDEN','ENGRENC','EPIGCON','EPIGDEN','EPIGTEL','EPINAEN','ETMOSPI','EUTRGUR','GADAMAR','GADIARG','GAIDBIS','GAIDMED','GALEGAL','GALUATL','GALUMEL','GLOSLEI','GNATMYS','GOBIFRI','GOBINIG','GOBIQUA','GOBISAN','GOBISUE','GONODEN','GYMNALT','HELIDAC','HEPTPER','HEXAGRI','HOPLMED','HYGOBEN','HYGOHIG','HYMEITA','LABRVIR','LAMACRO','LEPICAU','LEPMBOS','LEPMWHS','LEPTCAV','LEPTDIE','LITHMOR','LOBIDOF','LOPHSPP','MACOSCO','MAURMUE','MERLMER','MICMPOU','MICUOCE','MICUVAR','MOLVDYP','MOLVMOL','MONOHIS','MORAMOR','MULLBAR','MUSTAST','MUSTMUS','MYCOPUN','NETOBRE','NETTMEL','NEZUSCL','NOTSBOL','NOTSELO','OXYNCEN','PAGEACA','PAGEBEL','PAGEBOG','PAGEERY','PARALEP','PARLCOR','PERICAT','PHYIBLE','PHYSDAL','PLATFLE','POLYAME','POMSMAR','PSETMAX','RAJAALB','RAJAAST','RAJABRA','RAJACIR','RAJACLA','RAJAFUL','RAJAMEL','RAJAMIR','RAJAMON','RAJANAE','RAJAOXY','RAJAPOL','RAJAUND','SCHEOVA','SCOHRHO','SCORELO','SCORLOP','SCORNOT','SCORPOR','SCORSCO','SCYMLIC','SCYOCAN','SCYOSTE','SERACAB','SERAHEP','SOLEIMP','SOLEKLE','SOLELAS','SOLESEN','SOLEVUL','SPARPAG','SPICMAE','SPICSMA','SPODCAN','SQUAACA','SQUABLA','SQUTACU','SQUTOCL','SQUTSQU','STOMBOA','SYMBVER','SYMPLIG','SYMPNIG','SYNDSAU','TORPMAR','TORPNOB','TORPTOR','TRACMED','TRACPIC','TRACTRA','TRAHARA','TRAHDRA','TRAHRAD','TRARTRA','TRIGLUC','TRIGLYR','TRIPLAS','TRISCAP','UPENPOR','URANSCA','VINCPOW','ZENOCON','ZEUSFAB']\n",
    "indices_taxonomy=dict()\n",
    "c=0\n",
    "for i in l_taxonomy:\n",
    "    indices_taxonomy[i]=c\n",
    "    c=c+1\n",
    "matrice_taxonomy = np.loadtxt(\"matrice_taxo_esp.txt\")\n",
    "#print(matrice_taxonomy)\n",
    "\n",
    "\n",
    "\n",
    "l_phylogenetic=['ACATPAL','ALEPROS','ANTHANT','APHIMIN','ARGESPY','ARGRHEM','ARNOIMP','ARNOLAT','ARNORUP','ARNOTHO','AULOFIL','BASOPRO','BATHDUB','BENSGLA','BLENOCE','BOOPBOO','BOTHPOD','BUGLLUT','CALMLYR','CALMMAC','CALMPHA','CALMRIS','CAPOAPE','CECACIR','CENONIG','CEPOMAC','CERAMAD','CHAUSLO','CITHMAC','CLORAGA','DENTDEN','DIAPHOL','DIAPRAF','DIPLANN','DIPLVUL','ENGRENC','EPIGCON','EPIGDEN','EPIGTEL','EPINAEN','EUTRGUR','GADAMAR','GADIARG','GAIDBIS','GAIDMED','GLOSLEI','GNATMYS','GOBIFRI','GOBINIG','GOBIQUA','GOBISUE','GONODEN','HELIDAC','HOPLMED','HYGOBEN','HYGOHIG','HYMEITA','LABRVIR','LAMACRO','LEPICAU','LEPMBOS','LEPMWHS','LEPTCAV','LEPTDIE','LITHMOR','LOBIDOF','LOPHSPP','MACOSCO','MAURMUE','MERLMER','MICMPOU','MICUOCE','MICUVAR','MOLVDYP','MOLVMOL','MORAMOR','MULLBAR','MYCOPUN','NETOBRE','NETTMEL','NEZUSCL','NOTSBOL','NOTSELO','PAGEACA','PAGEBEL','PAGEBOG','PAGEERY','PARALEP','PARLCOR','PERICAT','PHYIBLE','PHYSDAL','PLATFLE','POLYAME','POMSMAR','PSETMAX','SCHEOVA','SCOHRHO','SCORELO','SCORLOP','SCORNOT','SCORPOR','SCORSCO','SERACAB','SERAHEP','SOLEIMP','SOLEKLE','SOLELAS','SOLESEN','SOLEVUL','SPARPAG','SPICMAE','SPICSMA','SPODCAN','STOMBOA','SYMBVER','SYMPLIG','SYMPNIG','SYNDSAU','TRACMED','TRACPIC','TRACTRA','TRAHARA','TRAHDRA','TRAHRAD','TRARTRA','TRIGLUC','TRIGLYR','TRIPLAS','TRISCAP','URANSCA','VINCPOW','ZEUSFAB']\n",
    "indices_phylogenetic=dict()\n",
    "c=0\n",
    "for i in l_phylogenetic:\n",
    "    indices_phylogenetic[i]=c\n",
    "    c=c+1\n",
    "matrice_phylogenetic = np.loadtxt(\"matrice_phylo_esp.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e556ba",
   "metadata": {},
   "source": [
    "# Data Extraction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "215d3b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the path to the path that leads to the folder that contains the dabase\n",
    "with open(r'C:/Users/zghalmane/Desktop/Codes/1Programme2/MEDITS_DataVF.dat') as y: \n",
    "    f=y.readlines()\n",
    "    #print(f)\n",
    "\n",
    "\n",
    "d=dict()  # date:\n",
    "k=0\n",
    "for ligne in f:\n",
    "    l=list()\n",
    "    n = ''\n",
    "    for i in ligne:\n",
    "        if (i!='\\t') and (i!=' ') and (i!='\\n') and (i!=''):\n",
    "            n = n + i\n",
    "        else:\n",
    "            if n!='':\n",
    "                l.append(n)\n",
    "                n=''\n",
    "    d[k]=l\n",
    "    k=k+1\n",
    "#print('d=',d) #dict(line nbr: what it contains)\n",
    "\n",
    "\n",
    "g=dict()\n",
    "for i in d:\n",
    "    g[i]=[]\n",
    "    for j in d[i]:\n",
    "        #print(j)\n",
    "        if isNumber(j):\n",
    "            g[i].append(float(j))\n",
    "        else:\n",
    "            g[i].append(j)\n",
    "#print('g=',g)  #dict(Line nbr: what it contains) transforming str to numbers\n",
    "#print(g[0])\n",
    "#print(g[0][172])\n",
    "\n",
    "g[27938]=[25,2019,97,28,34.67616667,33.14333333,0.095914498,0,0.09824228,0.028546499,0.108397576,0.163462802,0.000527733,0.000265663,0.021053605,0.01293833,39.20080957,0.020713998,17.54309371,0.413811942,0.00,0.00,0.00,0.00,761.09,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,15920.43,83.41,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,10.43,1834.97,0.00,0.00,10.43,0.00,0.00,0.00,0.00,1209.41,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,135.54,20.85,0.00,1136.43,667.26,0.00,20.85,31.28,0.00,0.00,0.00,0.00,0.00,156.39,62.56,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,479.59,20.85,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,10.43,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,10.43,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,104.26,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00]    \n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd19e1",
   "metadata": {},
   "source": [
    "# Initializing the gsa dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3bb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsa=dict()\n",
    "for i in l_gsa:\n",
    "    gsa[i]=dict()\n",
    "for i in gsa:\n",
    "    for j in years:\n",
    "        gsa[i][j]=dict()\n",
    "# print('gsa',gsa) #gsa=dict(gsa:{year:{():{specie 1: abd, specie 2: abd ...} ...} ...} ...)\n",
    "\n",
    "# fill the gsa dictionnary with values\n",
    "for i in g:\n",
    "    #print(i)\n",
    "    for j in g[i]:\n",
    "        d_species=dict()\n",
    "        k=0\n",
    "        for z in l_species:\n",
    "            d_species[z]=g[i][20+k]\n",
    "            k=k+1\n",
    "        gsa[g[i][0]][g[i][1]][tuple(g[i][2:20])]=d_species\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc13ecff",
   "metadata": {},
   "source": [
    "# Computing density of species = abundance/swept area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ass in gsa:\n",
    "    for t in gsa[ass]:\n",
    "        for e in gsa[ass][t]:\n",
    "            for i in gsa[ass][t][e]:\n",
    "                if (e[4]!='NA') and (e[4]!=0):\n",
    "                    gsa[ass][t][e][i]=float(gsa[ass][t][e][i])/e[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b28aa",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169041ed",
   "metadata": {},
   "source": [
    "# ****************************** Temporal Diversity & Temporal variation of species **********************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e91d3",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af72a83",
   "metadata": {},
   "source": [
    "# Co-occurrence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_gsa=[1, 6, 7, 9, 11, 16, 17, 18, 19]\n",
    "\n",
    "dict_size=dict()  #dict{gsa: {year: size}}\n",
    "dict_nbr_edges=dict()\n",
    "dict_density=dict()\n",
    "dict_diameter=dict()\n",
    "dict_transitivity=dict()\n",
    "dict_assortativity=dict()\n",
    "\n",
    "for index_gsa in l_gsa:\n",
    "    dict_size[index_gsa]=dict()  \n",
    "    dict_nbr_edges[index_gsa]=dict()\n",
    "    dict_density[index_gsa]=dict()\n",
    "    dict_diameter[index_gsa]=dict()\n",
    "    dict_transitivity[index_gsa]=dict()\n",
    "    dict_assortativity[index_gsa]=dict()\n",
    "\n",
    "dict_alpha=dict()\n",
    "for index_gsa in l_gsa:\n",
    "    \n",
    "    dict_new=dict()\n",
    "    for y in temps:\n",
    "        dict_new[y]=dict()    #dict(year: {specie: measure})\n",
    "    \n",
    "    for y in temps:\n",
    "        \n",
    "        gpa9=gsa[index_gsa][y]\n",
    "    \n",
    "        lgpa9=[] # list of species of each line which have a non null abd value [[specices with not null abd of line1], [spc ... line2] ...]\n",
    "        for i in gpa9:\n",
    "            l=[]\n",
    "            for j in gpa9[i]:\n",
    "                if(gpa9[i][j]!=0):\n",
    "                    l.append(j)\n",
    "            lgpa9.append(l)\n",
    "        #print('lgpa9=',lgpa9)\n",
    "    \n",
    "        lt=[] #lt=[construction of edges based on each line]\n",
    "        for l in lgpa9:\n",
    "            for i in range(0,len(l)-1):\n",
    "                for j in range(i+1,len(l)):\n",
    "                    lt.append((l[i],l[j]))\n",
    "        #print('lt=',lt)\n",
    "        #print(len(lt))\n",
    "    \n",
    "    \n",
    "        lt2=list(set(lt)) #lt2=[eliminate the double edges to obtain list with all the unique edges in the network]\n",
    "        #print('lt2=',lt2)\n",
    "        #print(len(lt2))\n",
    "        for i in range(0,len(lt2)-1):\n",
    "            for j in range(i+1,len(lt2)):\n",
    "                if (lt2[i][0]==lt2[j][1]) and (lt2[i][1]==lt2[j][0]):\n",
    "                    lt2.remove(lt2[j][0],lt2[j][1])\n",
    "    \n",
    "    \n",
    "        ################ Building Graph based on the nbr of selections #################\n",
    "        dict_lt=dict() #dict_lt{edge: number of selection}\n",
    "        for i in lt2:\n",
    "            dict_lt[i]=lt.count(i)\n",
    "    \n",
    "    \n",
    "        ltVF=[]\n",
    "        for i in dict_lt:\n",
    "            ltVF.append((i[0],i[1],dict_lt[i]))\n",
    "        #print('lt2=',lt2)\n",
    "    \n",
    "        g=nx.Graph()\n",
    "        g.add_weighted_edges_from(ltVF)\n",
    "        #size[gsa][y]=len(g)\n",
    "        #print(len(g))\n",
    "        #nbr_edges[gsa][y]=len(list(g.edges()))\n",
    "        dict_size[index_gsa][y]=len(g)  \n",
    "        dict_nbr_edges[index_gsa][y]=len(list(g.edges()))\n",
    "        dict_density[index_gsa][y]=nx.density(g)\n",
    "        dict_diameter[index_gsa][y]=nx.diameter(g)\n",
    "        dict_transitivity[index_gsa][y]=nx.transitivity(g)\n",
    "        dict_assortativity[index_gsa][y]=nx.degree_assortativity_coefficient(g)\n",
    "        ################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        dict_specie_abd=dict()\n",
    "        for sp in l_species:\n",
    "            dict_specie_abd[sp]=0\n",
    "                \n",
    "        # Data_gsa[i] : list of species of each line = gpa9\n",
    "        Data_gsa=gpa9\n",
    "            \n",
    "        for i in Data_gsa:\n",
    "            for j in Data_gsa[i]:\n",
    "                dict_specie_abd[j]=dict_specie_abd[j]+float(Data_gsa[i][j])\n",
    "        \n",
    "        edges=list(g.edges(data=True))\n",
    "        for e in edges:\n",
    "            #x=(2*e[2]['weight'])/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]])\n",
    "            #if x<=1:\n",
    "            #    print('hana')\n",
    "            e[2]['weight']=1-((abs(dict_specie_abd[e[0]]-dict_specie_abd[e[1]]))/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]]))\n",
    "            \n",
    "        #print(list(g.edges(data=True)))\n",
    "        \n",
    "        \n",
    "        dict_strength=dict()\n",
    "        l_strg=list()\n",
    "        for node in g:\n",
    "            s=0\n",
    "            l=g.edges(node, data=True)\n",
    "            for e in l:\n",
    "                s=s+e[2]['weight']\n",
    "            dict_strength[node]=s\n",
    "         \n",
    "        for node in g:\n",
    "            #x=dict_strength_fonction[node]*dict_strength[node]\n",
    "            x=dict_strength[node]\n",
    "            l_strg.append(x)\n",
    "            \n",
    "        for node in g:   \n",
    "            dict_strength[node]=dict_strength[node]/max(l_strg)\n",
    "        \n",
    "            \n",
    "        \n",
    "        dict_degree=nx.degree_centrality(g)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in dict_strength:\n",
    "            #dict_new[i]=dict_strength[i]/dict_degree[i]\n",
    "            dict_new[y][i]=math.sqrt(dict_strength[i]*dict_strength[i]+dict_degree[i]*dict_degree[i])\n",
    "            #if(dict_new[i]>=1):\n",
    "            #    print('hana')\n",
    "        \n",
    "        #print('dict_new',dict_new)\n",
    "        ptot=sum(list(dict_new[y].values()))\n",
    "        #print(ptot)\n",
    "            \n",
    "        for i in dict_new[y]:\n",
    "            if ptot==0:\n",
    "                dict_new[y][i]=0\n",
    "            else:\n",
    "                dict_new[y][i]=dict_new[y][i]/ptot\n",
    "                \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    d_species_abd_time=dict()  # dict(species:{t1: p1, t2: p2 ......})\n",
    "    for i in l_species:\n",
    "        d_species_abd_time[i]=dict()\n",
    "        for j in temps:\n",
    "            d_species_abd_time[i][j]=0\n",
    "            \n",
    "    \n",
    "    for i in l_species:\n",
    "        for y in temps:\n",
    "            Data_gsa=dict_new[y]\n",
    "            if list(Data_gsa).count(i)!=0:\n",
    "                d_species_abd_time[i][y]=Data_gsa[i]\n",
    "                \n",
    "    #print()\n",
    "    #print()\n",
    "    #print('d_species_abd_time', d_species_abd_time)\n",
    "\n",
    "\n",
    "\n",
    "    d_species_abdlist=dict()   #dict(species: {abd: nbr of years, })\n",
    "    for i in d_species_abd_time:\n",
    "        d_species_abdlist[i]=dict()\n",
    "    #print()\n",
    "    #print()\n",
    "    #print(d_species_abdlist)\n",
    "    for i in d_species_abd_time:\n",
    "        #print(d_species_abd_time)\n",
    "        l=list(d_species_abd_time[i].values())\n",
    "        #print()\n",
    "        #print()\n",
    "        #print(l)\n",
    "        list_abd=list(set(l))\n",
    "    \n",
    "        for j in list_abd:\n",
    "            d_species_abdlist[i][j]=l.count(j)\n",
    "\n",
    "\n",
    "    s=0\n",
    "    for i in d_species_abdlist:\n",
    "        for abd in d_species_abdlist[i]:\n",
    "            if abd!=0:\n",
    "                s=s-((d_species_abdlist[i][abd]/deltaT)*abd)*math.log(abd)\n",
    "    #print(s)\n",
    "    dict_alpha[index_gsa]=round(math.exp(s),2)\n",
    "\n",
    "\n",
    "for i in dict_alpha:\n",
    "    dict_alpha[i]=(dict_alpha[i]-1)/(q_MEDITS-1)\n",
    "\n",
    "print('Temporal diversity: ', dict_alpha) #Temporal diversity\n",
    "\n",
    "\"\"\"\n",
    "print()\n",
    "ig=1\n",
    "\n",
    "\n",
    "print(dict_size[ig])\n",
    "print(dict_nbr_edges[ig])\n",
    "print(dict_diameter[ig])\n",
    "print(dict_transitivity[ig])\n",
    "print(dict_density[ig])\n",
    "print(dict_assortativity[ig])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d94c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_alpha2=dict()\n",
    "dict_teta=dict()\n",
    "\n",
    "\n",
    "for index_gsa in l_gsa:\n",
    "    \n",
    "    dict_new=dict()\n",
    "    for y in temps:\n",
    "        dict_new[y]=dict()\n",
    "    \n",
    "    print()\n",
    "    l_sizes=list()\n",
    "    for y in temps:\n",
    "        \n",
    "        gpa9=gsa[index_gsa][y]\n",
    "    \n",
    "        lgpa9=[] # list of species of each line which have a non null abd value [[specices with not null abd of line1], [spc ... line2] ...]\n",
    "        for i in gpa9:\n",
    "            l=[]\n",
    "            for j in gpa9[i]:\n",
    "                if(gpa9[i][j]!=0):\n",
    "                    l.append(j)\n",
    "            lgpa9.append(l)\n",
    "        #print('lgpa9=',lgpa9)\n",
    "    \n",
    "        lt=[] #lt=[construction of edges based on each line]\n",
    "        for l in lgpa9:\n",
    "            for i in range(0,len(l)-1):\n",
    "                for j in range(i+1,len(l)):\n",
    "                    lt.append((l[i],l[j]))\n",
    "        #print('lt=',lt)\n",
    "        #print(len(lt))\n",
    "    \n",
    "    \n",
    "        lt2=list(set(lt)) #lt2=[eliminate the double edges to obtain list with all the unique edges in the network]\n",
    "        #print('lt2=',lt2)\n",
    "        #print(len(lt2))\n",
    "        for i in range(0,len(lt2)-1):\n",
    "            for j in range(i+1,len(lt2)):\n",
    "                if (lt2[i][0]==lt2[j][1]) and (lt2[i][1]==lt2[j][0]):\n",
    "                    lt2.remove(lt2[j][0],lt2[j][1])\n",
    "    \n",
    "    \n",
    "        ################ Building Graph based on the nbr of selections #################\n",
    "        dict_lt=dict() #dict_lt{edge: number of selection}\n",
    "        for i in lt2:\n",
    "            dict_lt[i]=lt.count(i)\n",
    "    \n",
    "    \n",
    "        ltVF=[]\n",
    "        for i in dict_lt:\n",
    "            ltVF.append((i[0],i[1],dict_lt[i]))\n",
    "        #print('lt2=',lt2)\n",
    "    \n",
    "        g=nx.Graph()\n",
    "        g.add_weighted_edges_from(ltVF)\n",
    "        #size[gsa][y]=len(g)\n",
    "        l_sizes.append(len(g))\n",
    "        #print(len(g))\n",
    "        #nbr_edges[gsa][y]=len(list(g.edges()))\n",
    "        ################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        dict_specie_abd=dict()\n",
    "        for sp in l_species:\n",
    "            dict_specie_abd[sp]=0\n",
    "                \n",
    "        # Data_gsa[i] : list of species of each line = gpa9\n",
    "        Data_gsa=gpa9\n",
    "            \n",
    "        for i in Data_gsa:\n",
    "            for j in Data_gsa[i]:\n",
    "                dict_specie_abd[j]=dict_specie_abd[j]+float(Data_gsa[i][j])\n",
    "        \n",
    "        edges=list(g.edges(data=True))\n",
    "        for e in edges:\n",
    "            #x=(2*e[2]['weight'])/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]])\n",
    "            #if x<=1:\n",
    "            #    print('hana')\n",
    "            e[2]['weight']=1-((abs(dict_specie_abd[e[0]]-dict_specie_abd[e[1]]))/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]]))\n",
    "            \n",
    "        #print(list(g.edges(data=True)))\n",
    "        \n",
    "        \n",
    "        dict_strength=dict()\n",
    "        l_strg=list()\n",
    "        for node in g:\n",
    "            s=0\n",
    "            l=g.edges(node, data=True)\n",
    "            for e in l:\n",
    "                s=s+e[2]['weight']\n",
    "            dict_strength[node]=s\n",
    "         \n",
    "        for node in g:\n",
    "            x=dict_strength[node]\n",
    "            l_strg.append(x)\n",
    "            \n",
    "        for node in g:   \n",
    "            dict_strength[node]=dict_strength[node]/max(l_strg)\n",
    "        \n",
    "            \n",
    "        \n",
    "        dict_degree=nx.degree_centrality(g)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in dict_strength:\n",
    "            #dict_new[i]=dict_strength[i]/dict_degree[i]\n",
    "            dict_new[y][i]=math.sqrt(dict_strength[i]*dict_strength[i]+dict_degree[i]*dict_degree[i])\n",
    "            #if(dict_new[i]>=1):\n",
    "            #    print('hana')\n",
    "        \n",
    "        #print('dict_new',dict_new)\n",
    "        #ptot=sum(list(dict_new[y].values()))\n",
    "        #print(ptot)\n",
    "        \n",
    "    list_of_nodes=set()\n",
    "    for y in temps:\n",
    "        for node in dict_new[y]:\n",
    "            list_of_nodes.add(node)\n",
    "    list_of_nodes=list(list_of_nodes)\n",
    "    \n",
    "    \n",
    "    ptot=dict()\n",
    "    for node in list_of_nodes:\n",
    "        ptot[node]=0\n",
    "    for node in list_of_nodes:\n",
    "        for y in temps:\n",
    "            if list(dict_new[y]).count(node)!=0:\n",
    "                ptot[node]=ptot[node]+dict_new[y][node]\n",
    "    \n",
    "    \n",
    "    dict_strength_connectivity=dict()\n",
    "    for y in temps:\n",
    "        dict_strength_connectivity[y]=dict()\n",
    "        for node in list_of_nodes:\n",
    "            dict_strength_connectivity[y][node]=0                    \n",
    "    for node in list_of_nodes:\n",
    "        for y in temps:\n",
    "            if ptot[node]==0:\n",
    "                dict_strength_connectivity[y][node]=0\n",
    "            else:\n",
    "                if list(dict_new[y]).count(node)==0:\n",
    "                    dict_strength_connectivity[y][node]=0\n",
    "                else:\n",
    "                    dict_strength_connectivity[y][node]=dict_new[y][node]/ptot[node]\n",
    "    \n",
    "    \n",
    "    div=dict()\n",
    "    for node in list_of_nodes:\n",
    "        div[node]=0\n",
    "    for node in list_of_nodes:\n",
    "        s=0\n",
    "        for y in temps:\n",
    "            p=dict_strength_connectivity[y][node]\n",
    "            if p!=0:\n",
    "                s=s-p*math.log(p)\n",
    "        div[node]=math.exp(s)\n",
    "        \n",
    "    divT=sum(list(div.values()))\n",
    "    #print(l_sizes)\n",
    "    #print(max(l_sizes))\n",
    "    #print(len(list_of_nodes))\n",
    "    #print()\n",
    "    #print(index_gsa)\n",
    "    #print(divT)\n",
    "    #print(len(list_of_nodes))\n",
    "    teta=1-(divT-len(list_of_nodes))/(len(list_of_nodes)*(len(temps)-1))\n",
    "    \n",
    "    dict_alpha2[index_gsa]=round(divT,2)    \n",
    "    dict_teta[index_gsa]=round(teta,2)   \n",
    "\n",
    "\n",
    "#print(dict_alpha2)\n",
    "print('Temporal variation of species :',dict_teta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520dd4a",
   "metadata": {},
   "source": [
    "# Combination Co-occurence & Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_alpha_functional_coccurrence=dict()\n",
    "for index_gsa in l_gsa:\n",
    "    \n",
    "    dict_new=dict()\n",
    "    for y in temps:\n",
    "        dict_new[y]=dict()    #dict(year: {specie: measure})\n",
    "        \n",
    "    for y in temps:\n",
    "        \n",
    "        gpa9=gsa[index_gsa][y]\n",
    "    \n",
    "        lgpa9=[] # list of species of each line which have a non null abd value [[specices with not null abd of line1], [spc ... line2] ...]\n",
    "        for i in gpa9:\n",
    "            l=[]\n",
    "            for j in gpa9[i]:\n",
    "                if(gpa9[i][j]!=0):\n",
    "                    l.append(j)\n",
    "            lgpa9.append(l)\n",
    "        #print('lgpa9=',lgpa9)\n",
    "    \n",
    "        lt=[] #lt=[construction of edges based on each line]\n",
    "        for l in lgpa9:\n",
    "            for i in range(0,len(l)-1):\n",
    "                for j in range(i+1,len(l)):\n",
    "                    lt.append((l[i],l[j]))\n",
    "        #print('lt=',lt)\n",
    "        #print(len(lt))\n",
    "    \n",
    "    \n",
    "        lt2=list(set(lt)) #lt2=[eliminate the double edges to obtain list with all the unique edges in the network]\n",
    "        #print('lt2=',lt2)\n",
    "        #print(len(lt2))\n",
    "        for i in range(0,len(lt2)-1):\n",
    "            for j in range(i+1,len(lt2)):\n",
    "                if (lt2[i][0]==lt2[j][1]) and (lt2[i][1]==lt2[j][0]):\n",
    "                    lt2.remove(lt2[j][0],lt2[j][1])\n",
    "    \n",
    "    \n",
    "        ################ Building Graph based on the nbr of selections #################\n",
    "        dict_lt=dict() #dict_lt{edge: number of selection}\n",
    "        for i in lt2:\n",
    "            dict_lt[i]=lt.count(i)\n",
    "    \n",
    "    \n",
    "        ltVF=[]\n",
    "        for i in dict_lt:\n",
    "            ltVF.append((i[0],i[1],dict_lt[i]))\n",
    "        #print('lt2=',lt2)\n",
    "    \n",
    "        g=nx.Graph()\n",
    "        g.add_weighted_edges_from(ltVF)\n",
    "        #size[gsa][y]=len(g)\n",
    "        #print(len(g))\n",
    "        #nbr_edges[gsa][y]=len(list(g.edges()))\n",
    "        ################################################################################\n",
    "    \n",
    "    \n",
    "        ################ Building Graph based on functional matrix #####################\n",
    "        dict_lt_fonction=dict() #dict_lt_fonction{edge: function distance}\n",
    "        for i in lt2:\n",
    "            dict_lt_fonction[i]=matrice_fonction[indices_fonction[i[0]]][indices_fonction[i[1]]]\n",
    "    \n",
    "    \n",
    "        nbr_removed=0\n",
    "        ltVF=[]\n",
    "        for i in dict_lt_fonction:\n",
    "            if (dict_lt_fonction[i]==0):\n",
    "                nbr_removed=nbr_removed+1\n",
    "            else:\n",
    "                ltVF.append((i[0],i[1],dict_lt_fonction[i]))\n",
    "        #print('lt2=',lt2)\n",
    "    \n",
    "        g_fonction=nx.Graph()\n",
    "        g_fonction.add_weighted_edges_from(ltVF)\n",
    "        ################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        dict_specie_abd=dict()\n",
    "        for sp in l_species:\n",
    "            dict_specie_abd[sp]=0\n",
    "                \n",
    "        # Data_gsa[i] : list of species of each line = gpa9\n",
    "        Data_gsa=gpa9\n",
    "            \n",
    "        for i in Data_gsa:\n",
    "            for j in Data_gsa[i]:\n",
    "                dict_specie_abd[j]=dict_specie_abd[j]+float(Data_gsa[i][j])\n",
    "        \n",
    "        edges=list(g.edges(data=True))\n",
    "        for e in edges:\n",
    "            #x=(2*e[2]['weight'])/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]])\n",
    "            #if x<=1:\n",
    "            #    print('hana')\n",
    "            e[2]['weight']=1-((abs(dict_specie_abd[e[0]]-dict_specie_abd[e[1]]))/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]]))\n",
    "            \n",
    "        #print(list(g.edges(data=True)))\n",
    "        \n",
    "        \n",
    "        dict_strength=dict()\n",
    "        l_strg=list()\n",
    "        for node in g:\n",
    "            s=0\n",
    "            l=g.edges(node, data=True)\n",
    "            for e in l:\n",
    "                s=s+e[2]['weight']\n",
    "            dict_strength[node]=s\n",
    "            \n",
    "        dict_strength_fonction=dict()\n",
    "        for node in g_fonction:\n",
    "            s=0\n",
    "            l=g_fonction.edges(node, data=True)\n",
    "            for e in l:\n",
    "                s=s+e[2]['weight']\n",
    "            dict_strength_fonction[node]=s\n",
    "         \n",
    "        for node in g_fonction:\n",
    "            x=dict_strength_fonction[node]*dict_strength[node]\n",
    "            dict_strength_fonction[node]=x\n",
    "            l_strg.append(x)\n",
    "            \n",
    "        for node in g_fonction:   \n",
    "            dict_strength_fonction[node]=dict_strength_fonction[node]/max(l_strg)\n",
    "        \n",
    "            \n",
    "        \n",
    "        dict_degree=nx.degree_centrality(g_fonction)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in dict_strength_fonction:\n",
    "            #dict_new[i]=dict_strength[i]/dict_degree[i]\n",
    "            dict_new[y][i]=math.sqrt(dict_strength_fonction[i]*dict_strength_fonction[i]+dict_degree[i]*dict_degree[i])\n",
    "            #if(dict_new[i]>=1):\n",
    "            #    print('hana')\n",
    "        \n",
    "        #print('dict_new',dict_new)\n",
    "        ptot=sum(list(dict_new[y].values()))\n",
    "        #print(ptot)\n",
    "            \n",
    "        for i in dict_new[y]:\n",
    "            if ptot==0:\n",
    "                dict_new[y][i]=0\n",
    "            else:\n",
    "                dict_new[y][i]=dict_new[y][i]/ptot\n",
    "                \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    d_species_abd_time=dict()  # dict(species:{t1: p1, t2: p2 ......})\n",
    "    for i in l_species:\n",
    "        d_species_abd_time[i]=dict()\n",
    "        for j in temps:\n",
    "            d_species_abd_time[i][j]=0\n",
    "            \n",
    "    \n",
    "    for i in l_species:\n",
    "        for y in temps:\n",
    "            Data_gsa=dict_new[y]\n",
    "            if list(Data_gsa).count(i)!=0:\n",
    "                d_species_abd_time[i][y]=Data_gsa[i]\n",
    "                \n",
    "    #print()\n",
    "    #print()\n",
    "    #print('d_species_abd_time', d_species_abd_time)\n",
    "\n",
    "\n",
    "\n",
    "    d_species_abdlist=dict()   #dict(species: {abd: nbr of years, })\n",
    "    for i in d_species_abd_time:\n",
    "        d_species_abdlist[i]=dict()\n",
    "    #print()\n",
    "    #print()\n",
    "    #print(d_species_abdlist)\n",
    "    for i in d_species_abd_time:\n",
    "        #print(d_species_abd_time)\n",
    "        l=list(d_species_abd_time[i].values())\n",
    "        #print()\n",
    "        #print()\n",
    "        #print(l)\n",
    "        list_abd=list(set(l))\n",
    "    \n",
    "        for j in list_abd:\n",
    "            d_species_abdlist[i][j]=l.count(j)\n",
    "\n",
    "\n",
    "    s=0\n",
    "    for i in d_species_abdlist:\n",
    "        for abd in d_species_abdlist[i]:\n",
    "            if abd!=0:\n",
    "                s=s-((d_species_abdlist[i][abd]/deltaT)*abd)*math.log(abd)\n",
    "    #print(s)\n",
    "    dict_alpha_functional_coccurrence[index_gsa]=round(math.exp(s),2)\n",
    "\n",
    "    \n",
    "for i in dict_alpha_functional_coccurrence:\n",
    "    dict_alpha_functional_coccurrence[i]=(dict_alpha_functional_coccurrence[i]-1)/(q_MEDITS-1)\n",
    "\n",
    "print(\"Temporal Diversity: \", dict_alpha_functional_coccurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_alpha_functional_coccurrence2=dict()\n",
    "dict_teta_functional_coccurrence=dict()\n",
    "\n",
    "for index_gsa in l_gsa:\n",
    "    \n",
    "    dict_new=dict()\n",
    "    for y in temps:\n",
    "        dict_new[y]=dict()    #dict(year: {specie: measure})\n",
    "        \n",
    "    print()\n",
    "    for y in temps:\n",
    "        \n",
    "        gpa9=gsa[index_gsa][y]\n",
    "    \n",
    "        lgpa9=[] # list of species of each line which have a non null abd value [[specices with not null abd of line1], [spc ... line2] ...]\n",
    "        for i in gpa9:\n",
    "            l=[]\n",
    "            for j in gpa9[i]:\n",
    "                if(gpa9[i][j]!=0):\n",
    "                    l.append(j)\n",
    "            lgpa9.append(l)\n",
    "        #print('lgpa9=',lgpa9)\n",
    "    \n",
    "        lt=[] #lt=[construction of edges based on each line]\n",
    "        for l in lgpa9:\n",
    "            for i in range(0,len(l)-1):\n",
    "                for j in range(i+1,len(l)):\n",
    "                    lt.append((l[i],l[j]))\n",
    "        #print('lt=',lt)\n",
    "        #print(len(lt))\n",
    "    \n",
    "    \n",
    "        lt2=list(set(lt)) #lt2=[eliminate the double edges to obtain list with all the unique edges in the network]\n",
    "        #print('lt2=',lt2)\n",
    "        #print(len(lt2))\n",
    "        for i in range(0,len(lt2)-1):\n",
    "            for j in range(i+1,len(lt2)):\n",
    "                if (lt2[i][0]==lt2[j][1]) and (lt2[i][1]==lt2[j][0]):\n",
    "                    lt2.remove(lt2[j][0],lt2[j][1])\n",
    "    \n",
    "    \n",
    "        ################ Building Graph based on the nbr of selections #################\n",
    "        dict_lt=dict() #dict_lt{edge: number of selection}\n",
    "        for i in lt2:\n",
    "            dict_lt[i]=lt.count(i)\n",
    "    \n",
    "    \n",
    "        ltVF=[]\n",
    "        for i in dict_lt:\n",
    "            ltVF.append((i[0],i[1],dict_lt[i]))\n",
    "        #print('lt2=',lt2)\n",
    "    \n",
    "        g=nx.Graph()\n",
    "        g.add_weighted_edges_from(ltVF)\n",
    "        #size[gsa][y]=len(g)\n",
    "        #print(len(g))\n",
    "        #nbr_edges[gsa][y]=len(list(g.edges()))\n",
    "        ################################################################################\n",
    "    \n",
    "    \n",
    "        ################ Building Graph based on functional matrix #####################\n",
    "        dict_lt_fonction=dict() #dict_lt_fonction{edge: function distance}\n",
    "        for i in lt2:\n",
    "            dict_lt_fonction[i]=matrice_fonction[indices_fonction[i[0]]][indices_fonction[i[1]]]\n",
    "    \n",
    "    \n",
    "        nbr_removed=0\n",
    "        ltVF=[]\n",
    "        for i in dict_lt_fonction:\n",
    "            if (dict_lt_fonction[i]==0):\n",
    "                nbr_removed=nbr_removed+1\n",
    "            else:\n",
    "                ltVF.append((i[0],i[1],dict_lt_fonction[i]))\n",
    "        #print('lt2=',lt2)\n",
    "    \n",
    "        g_fonction=nx.Graph()\n",
    "        g_fonction.add_weighted_edges_from(ltVF)\n",
    "        ################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        dict_specie_abd=dict()\n",
    "        for sp in l_species:\n",
    "            dict_specie_abd[sp]=0\n",
    "                \n",
    "        # Data_gsa[i] : list of species of each line = gpa9\n",
    "        Data_gsa=gpa9\n",
    "            \n",
    "        for i in Data_gsa:\n",
    "            for j in Data_gsa[i]:\n",
    "                dict_specie_abd[j]=dict_specie_abd[j]+float(Data_gsa[i][j])\n",
    "        \n",
    "        edges=list(g.edges(data=True))\n",
    "        for e in edges:\n",
    "            #x=(2*e[2]['weight'])/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]])\n",
    "            #if x<=1:\n",
    "            #    print('hana')\n",
    "            e[2]['weight']=1-((abs(dict_specie_abd[e[0]]-dict_specie_abd[e[1]]))/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]]))\n",
    "            \n",
    "        #print(list(g.edges(data=True)))\n",
    "        \n",
    "        \n",
    "        dict_strength=dict()\n",
    "        l_strg=list()\n",
    "        for node in g:\n",
    "            s=0\n",
    "            l=g.edges(node, data=True)\n",
    "            for e in l:\n",
    "                s=s+e[2]['weight']\n",
    "            dict_strength[node]=s\n",
    "            \n",
    "        dict_strength_fonction=dict()\n",
    "        for node in g_fonction:\n",
    "            s=0\n",
    "            l=g_fonction.edges(node, data=True)\n",
    "            for e in l:\n",
    "                s=s+e[2]['weight']\n",
    "            dict_strength_fonction[node]=s\n",
    "         \n",
    "        for node in g_fonction:\n",
    "            x=dict_strength_fonction[node]*dict_strength[node]\n",
    "            dict_strength_fonction[node]=x\n",
    "            l_strg.append(x)\n",
    "            \n",
    "        for node in g_fonction:   \n",
    "            dict_strength_fonction[node]=dict_strength_fonction[node]/max(l_strg)\n",
    "        \n",
    "            \n",
    "        \n",
    "        dict_degree=nx.degree_centrality(g_fonction)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in dict_strength_fonction:\n",
    "            #dict_new[i]=dict_strength[i]/dict_degree[i]\n",
    "            dict_new[y][i]=math.sqrt(dict_strength_fonction[i]*dict_strength_fonction[i]+dict_degree[i]*dict_degree[i])\n",
    "            #if(dict_new[i]>=1):\n",
    "            #    print('hana')\n",
    "        \n",
    "        #print('dict_new',dict_new)\n",
    "        #ptot=sum(list(dict_new[y].values()))\n",
    "        #print(ptot)\n",
    "        \n",
    "    list_of_nodes=set()\n",
    "    for y in temps:\n",
    "        for node in dict_new[y]:\n",
    "            list_of_nodes.add(node)\n",
    "    list_of_nodes=list(list_of_nodes)\n",
    "    \n",
    "    \n",
    "    ptot=dict()\n",
    "    for node in list_of_nodes:\n",
    "        ptot[node]=0\n",
    "    for node in list_of_nodes:\n",
    "        for y in temps:\n",
    "            if list(dict_new[y]).count(node)!=0:\n",
    "                ptot[node]=ptot[node]+dict_new[y][node]\n",
    "    \n",
    "    \n",
    "    dict_strength_connectivity=dict()\n",
    "    for y in temps:\n",
    "        dict_strength_connectivity[y]=dict()\n",
    "        for node in list_of_nodes:\n",
    "            dict_strength_connectivity[y][node]=0                    \n",
    "    for node in list_of_nodes:\n",
    "        for y in temps:\n",
    "            if ptot[node]==0:\n",
    "                dict_strength_connectivity[y][node]=0\n",
    "            else:\n",
    "                if list(dict_new[y]).count(node)==0:\n",
    "                    dict_strength_connectivity[y][node]=0\n",
    "                else:\n",
    "                    dict_strength_connectivity[y][node]=dict_new[y][node]/ptot[node]\n",
    "    \n",
    "    \n",
    "    div=dict()\n",
    "    for node in list_of_nodes:\n",
    "        div[node]=0\n",
    "    for node in list_of_nodes:\n",
    "        s=0\n",
    "        for y in temps:\n",
    "            p=dict_strength_connectivity[y][node]\n",
    "            if p!=0:\n",
    "                s=s-p*math.log(p)\n",
    "        div[node]=math.exp(s)\n",
    "        \n",
    "    divT=sum(list(div.values()))\n",
    "    teta=1-(divT-len(list_of_nodes))/(len(list_of_nodes)*(len(temps)-1))\n",
    "    \n",
    "    dict_alpha_functional_coccurrence2[index_gsa]=round(divT,2)    \n",
    "    dict_teta_functional_coccurrence[index_gsa]=round(teta,2)   \n",
    "\n",
    "\n",
    "#print(dict_alpha_functional_coccurrence2)\n",
    "print('Temporal variation of species: 'dict_teta_functional_coccurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47bce6",
   "metadata": {},
   "source": [
    "# Combination Co-occurence & Taxonomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83359d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_alpha_taxonomic_coccurrence=dict()\n",
    "for index_gsa in l_gsa:\n",
    "    \n",
    "    dict_new=dict()\n",
    "    for y in temps:\n",
    "        dict_new[y]=dict()    #dict(year: {specie: measure})\n",
    "        \n",
    "    for y in temps:\n",
    "        \n",
    "        gpa9=gsa[index_gsa][y]\n",
    "    \n",
    "        lgpa9=[] # list of species of each line which have a non null abd value [[specices with not null abd of line1], [spc ... line2] ...]\n",
    "        for i in gpa9:\n",
    "            l=[]\n",
    "            for j in gpa9[i]:\n",
    "                if(gpa9[i][j]!=0):\n",
    "                    l.append(j)\n",
    "            lgpa9.append(l)\n",
    "        #print('lgpa9=',lgpa9)\n",
    "    \n",
    "        lt=[] #lt=[construction of edges based on each line]\n",
    "        for l in lgpa9:\n",
    "            for i in range(0,len(l)-1):\n",
    "                for j in range(i+1,len(l)):\n",
    "                    lt.append((l[i],l[j]))\n",
    "        #print('lt=',lt)\n",
    "        #print(len(lt))\n",
    "    \n",
    "    \n",
    "        lt2=list(set(lt)) #lt2=[eliminate the double edges to obtain list with all the unique edges in the network]\n",
    "        #print('lt2=',lt2)\n",
    "        #print(len(lt2))\n",
    "        for i in range(0,len(lt2)-1):\n",
    "            for j in range(i+1,len(lt2)):\n",
    "                if (lt2[i][0]==lt2[j][1]) and (lt2[i][1]==lt2[j][0]):\n",
    "                    lt2.remove(lt2[j][0],lt2[j][1])\n",
    "    \n",
    "    \n",
    "        ################ Building Graph based on the nbr of selections #################\n",
    "        dict_lt=dict() #dict_lt{edge: number of selection}\n",
    "        for i in lt2:\n",
    "            dict_lt[i]=lt.count(i)\n",
    "    \n",
    "    \n",
    "        ltVF=[]\n",
    "        for i in dict_lt:\n",
    "            ltVF.append((i[0],i[1],dict_lt[i]))\n",
    "        #print('lt2=',lt2)\n",
    "    \n",
    "        g=nx.Graph()\n",
    "        g.add_weighted_edges_from(ltVF)\n",
    "        #size[gsa][y]=len(g)\n",
    "        #print(len(g))\n",
    "        #nbr_edges[gsa][y]=len(list(g.edges()))\n",
    "        ################################################################################\n",
    "    \n",
    "    \n",
    "        ################ Building Graph based on taxonomic matrix ######################\n",
    "        dict_lt_taxonomy=dict() #dict_lt_taxonomy{edge: taxonomy distance}\n",
    "        for i in lt2:\n",
    "            dict_lt_taxonomy[i]=matrice_taxonomy[indices_taxonomy[i[0]]][indices_taxonomy[i[1]]]\n",
    "    \n",
    "    \n",
    "        nbr_removed=0\n",
    "        ltVF=[]\n",
    "        for i in dict_lt_taxonomy:\n",
    "            if (dict_lt_taxonomy[i]==0):\n",
    "                nbr_removed=nbr_removed+1\n",
    "            else:\n",
    "                ltVF.append((i[0],i[1],dict_lt_taxonomy[i]))\n",
    "        #print('lt2=',lt2)\n",
    "    \n",
    "        g_taxonomy=nx.Graph()\n",
    "        g_taxonomy.add_weighted_edges_from(ltVF)\n",
    "        ################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        dict_specie_abd=dict()\n",
    "        for sp in l_species:\n",
    "            dict_specie_abd[sp]=0\n",
    "                \n",
    "        # Data_gsa[i] : list of species of each line = gpa9\n",
    "        Data_gsa=gpa9\n",
    "            \n",
    "        for i in Data_gsa:\n",
    "            for j in Data_gsa[i]:\n",
    "                dict_specie_abd[j]=dict_specie_abd[j]+float(Data_gsa[i][j])\n",
    "        \n",
    "        edges=list(g.edges(data=True))\n",
    "        for e in edges:\n",
    "            #x=(2*e[2]['weight'])/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]])\n",
    "            #if x<=1:\n",
    "            #    print('hana')\n",
    "            e[2]['weight']=1-((abs(dict_specie_abd[e[0]]-dict_specie_abd[e[1]]))/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]]))\n",
    "            \n",
    "        #print(list(g.edges(data=True)))\n",
    "        \n",
    "        \n",
    "        dict_strength=dict()\n",
    "        l_strg=list()\n",
    "        for node in g:\n",
    "            s=0\n",
    "            l=g.edges(node, data=True)\n",
    "            for e in l:\n",
    "                s=s+e[2]['weight']\n",
    "            dict_strength[node]=s\n",
    "            \n",
    "        dict_strength_taxonomy=dict()\n",
    "        for node in g_taxonomy:\n",
    "            s=0\n",
    "            l=g_taxonomy.edges(node, data=True)\n",
    "            for e in l:\n",
    "                s=s+e[2]['weight']\n",
    "            dict_strength_taxonomy[node]=s\n",
    "         \n",
    "        for node in g_taxonomy:\n",
    "            x=dict_strength_taxonomy[node]*dict_strength[node]\n",
    "            dict_strength_taxonomy[node]=x\n",
    "            l_strg.append(x)\n",
    "            \n",
    "        for node in g_taxonomy:   \n",
    "            dict_strength_taxonomy[node]=dict_strength_taxonomy[node]/max(l_strg)\n",
    "        \n",
    "            \n",
    "        \n",
    "        dict_degree=nx.degree_centrality(g_taxonomy)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in dict_strength_taxonomy:\n",
    "            #dict_new[i]=dict_strength[i]/dict_degree[i]\n",
    "            dict_new[y][i]=math.sqrt(dict_strength_taxonomy[i]*dict_strength_taxonomy[i]+dict_degree[i]*dict_degree[i])\n",
    "            #if(dict_new[i]>=1):\n",
    "            #    print('hana')\n",
    "        \n",
    "        #print('dict_new',dict_new)\n",
    "        ptot=sum(list(dict_new[y].values()))\n",
    "        #print(ptot)\n",
    "            \n",
    "        for i in dict_new[y]:\n",
    "            if ptot==0:\n",
    "                dict_new[y][i]=0\n",
    "            else:\n",
    "                dict_new[y][i]=dict_new[y][i]/ptot\n",
    "                \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    d_species_abd_time=dict()  # dict(species:{t1: p1, t2: p2 ......})\n",
    "    for i in l_species:\n",
    "        d_species_abd_time[i]=dict()\n",
    "        for j in temps:\n",
    "            d_species_abd_time[i][j]=0\n",
    "            \n",
    "    \n",
    "    for i in l_species:\n",
    "        for y in temps:\n",
    "            Data_gsa=dict_new[y]\n",
    "            if list(Data_gsa).count(i)!=0:\n",
    "                d_species_abd_time[i][y]=Data_gsa[i]\n",
    "                \n",
    "    #print()\n",
    "    #print()\n",
    "    #print('d_species_abd_time', d_species_abd_time)\n",
    "\n",
    "\n",
    "\n",
    "    d_species_abdlist=dict()   #dict(species: {abd: nbr of years, })\n",
    "    for i in d_species_abd_time:\n",
    "        d_species_abdlist[i]=dict()\n",
    "    #print()\n",
    "    #print()\n",
    "    #print(d_species_abdlist)\n",
    "    for i in d_species_abd_time:\n",
    "        #print(d_species_abd_time)\n",
    "        l=list(d_species_abd_time[i].values())\n",
    "        #print()\n",
    "        #print()\n",
    "        #print(l)\n",
    "        list_abd=list(set(l))\n",
    "    \n",
    "        for j in list_abd:\n",
    "            d_species_abdlist[i][j]=l.count(j)\n",
    "\n",
    "\n",
    "    s=0\n",
    "    for i in d_species_abdlist:\n",
    "        for abd in d_species_abdlist[i]:\n",
    "            if abd!=0:\n",
    "                s=s-((d_species_abdlist[i][abd]/deltaT)*abd)*math.log(abd)\n",
    "    #print(s)\n",
    "    dict_alpha_taxonomic_coccurrence[index_gsa]=round(math.exp(s),2)\n",
    "\n",
    "    \n",
    "for i in dict_alpha_taxonomic_coccurrence:\n",
    "    dict_alpha_taxonomic_coccurrence[i]=(dict_alpha_taxonomic_coccurrence[i]-1)/(q_MEDITS-1)\n",
    "    \n",
    "print('Temporal Diversity: ',dict_alpha_taxonomic_coccurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f82ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_alpha_taxonomic_coccurrence2=dict()\n",
    "dict_teta_taxonomic_coccurrence=dict()\n",
    "\n",
    "for index_gsa in l_gsa:\n",
    "    \n",
    "    dict_new=dict()\n",
    "    for y in temps:\n",
    "        dict_new[y]=dict()    #dict(year: {specie: measure})\n",
    "        \n",
    "    print()\n",
    "    for y in temps:\n",
    "        \n",
    "        gpa9=gsa[index_gsa][y]\n",
    "    \n",
    "        lgpa9=[] # list of species of each line which have a non null abd value [[specices with not null abd of line1], [spc ... line2] ...]\n",
    "        for i in gpa9:\n",
    "            l=[]\n",
    "            for j in gpa9[i]:\n",
    "                if(gpa9[i][j]!=0):\n",
    "                    l.append(j)\n",
    "            lgpa9.append(l)\n",
    "        #print('lgpa9=',lgpa9)\n",
    "    \n",
    "        lt=[] #lt=[construction of edges based on each line]\n",
    "        for l in lgpa9:\n",
    "            for i in range(0,len(l)-1):\n",
    "                for j in range(i+1,len(l)):\n",
    "                    lt.append((l[i],l[j]))\n",
    "        #print('lt=',lt)\n",
    "        #print(len(lt))\n",
    "    \n",
    "    \n",
    "        lt2=list(set(lt)) #lt2=[eliminate the double edges to obtain list with all the unique edges in the network]\n",
    "        #print('lt2=',lt2)\n",
    "        #print(len(lt2))\n",
    "        for i in range(0,len(lt2)-1):\n",
    "            for j in range(i+1,len(lt2)):\n",
    "                if (lt2[i][0]==lt2[j][1]) and (lt2[i][1]==lt2[j][0]):\n",
    "                    lt2.remove(lt2[j][0],lt2[j][1])\n",
    "    \n",
    "    \n",
    "        ################ Building Graph based on the nbr of selections #################\n",
    "        dict_lt=dict() #dict_lt{edge: number of selection}\n",
    "        for i in lt2:\n",
    "            dict_lt[i]=lt.count(i)\n",
    "    \n",
    "    \n",
    "        ltVF=[]\n",
    "        for i in dict_lt:\n",
    "            ltVF.append((i[0],i[1],dict_lt[i]))\n",
    "        #print('lt2=',lt2)\n",
    "    \n",
    "        g=nx.Graph()\n",
    "        g.add_weighted_edges_from(ltVF)\n",
    "        #size[gsa][y]=len(g)\n",
    "        #print(len(g))\n",
    "        #nbr_edges[gsa][y]=len(list(g.edges()))\n",
    "        ################################################################################\n",
    "    \n",
    "    \n",
    "        ################ Building Graph based on taxonomic matrix ######################\n",
    "        dict_lt_taxonomy=dict() #dict_lt_taxonomy{edge: taxonomy distance}\n",
    "        for i in lt2:\n",
    "            dict_lt_taxonomy[i]=matrice_taxonomy[indices_taxonomy[i[0]]][indices_taxonomy[i[1]]]\n",
    "    \n",
    "    \n",
    "        nbr_removed=0\n",
    "        ltVF=[]\n",
    "        for i in dict_lt_taxonomy:\n",
    "            if (dict_lt_taxonomy[i]==0):\n",
    "                nbr_removed=nbr_removed+1\n",
    "            else:\n",
    "                ltVF.append((i[0],i[1],dict_lt_taxonomy[i]))\n",
    "        #print('lt2=',lt2)\n",
    "    \n",
    "        g_taxonomy=nx.Graph()\n",
    "        g_taxonomy.add_weighted_edges_from(ltVF)\n",
    "        ################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        dict_specie_abd=dict()\n",
    "        for sp in l_species:\n",
    "            dict_specie_abd[sp]=0\n",
    "                \n",
    "        # Data_gsa[i] : list of species of each line = gpa9\n",
    "        Data_gsa=gpa9\n",
    "            \n",
    "        for i in Data_gsa:\n",
    "            for j in Data_gsa[i]:\n",
    "                dict_specie_abd[j]=dict_specie_abd[j]+float(Data_gsa[i][j])\n",
    "        \n",
    "        edges=list(g.edges(data=True))\n",
    "        for e in edges:\n",
    "            #x=(2*e[2]['weight'])/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]])\n",
    "            #if x<=1:9\n",
    "            #    print('hana')\n",
    "            e[2]['weight']=1-((abs(dict_specie_abd[e[0]]-dict_specie_abd[e[1]]))/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]]))\n",
    "            \n",
    "        #print(list(g.edges(data=True)))\n",
    "        \n",
    "        \n",
    "        dict_strength=dict()\n",
    "        l_strg=list()\n",
    "        for node in g:\n",
    "            s=0\n",
    "            l=g.edges(node, data=True)\n",
    "            for e in l:\n",
    "                s=s+e[2]['weight']\n",
    "            dict_strength[node]=s\n",
    "            \n",
    "        dict_strength_taxonomy=dict()\n",
    "        for node in g_taxonomy:\n",
    "            s=0\n",
    "            l=g_taxonomy.edges(node, data=True)\n",
    "            for e in l:\n",
    "                s=s+e[2]['weight']\n",
    "            dict_strength_taxonomy[node]=s\n",
    "         \n",
    "        for node in g_taxonomy:\n",
    "            x=dict_strength_taxonomy[node]*dict_strength[node]\n",
    "            dict_strength_taxonomy[node]=x\n",
    "            l_strg.append(x)\n",
    "            \n",
    "        for node in g_taxonomy:   \n",
    "            dict_strength_taxonomy[node]=dict_strength_taxonomy[node]/max(l_strg)\n",
    "        \n",
    "            \n",
    "        \n",
    "        dict_degree=nx.degree_centrality(g_taxonomy)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in dict_strength_taxonomy:\n",
    "            #dict_new[i]=dict_strength[i]/dict_degree[i]\n",
    "            dict_new[y][i]=math.sqrt(dict_strength_taxonomy[i]*dict_strength_taxonomy[i]+dict_degree[i]*dict_degree[i])\n",
    "            #if(dict_new[i]>=1):\n",
    "            #    print('hana')\n",
    "        \n",
    "        #print('dict_new',dict_new)\n",
    "        #ptot=sum(list(dict_new[y].values()))\n",
    "        #print(ptot)\n",
    "        \n",
    "    list_of_nodes=set()\n",
    "    for y in temps:\n",
    "        for node in dict_new[y]:\n",
    "            list_of_nodes.add(node)\n",
    "    list_of_nodes=list(list_of_nodes)\n",
    "    \n",
    "    \n",
    "    ptot=dict()\n",
    "    for node in list_of_nodes:\n",
    "        ptot[node]=0\n",
    "    for node in list_of_nodes:\n",
    "        for y in temps:\n",
    "            if list(dict_new[y]).count(node)!=0:\n",
    "                ptot[node]=ptot[node]+dict_new[y][node]\n",
    "    \n",
    "    \n",
    "    dict_strength_connectivity=dict()\n",
    "    for y in temps:\n",
    "        dict_strength_connectivity[y]=dict()\n",
    "        for node in list_of_nodes:\n",
    "            dict_strength_connectivity[y][node]=0                    \n",
    "    for node in list_of_nodes:\n",
    "        for y in temps:\n",
    "            if ptot[node]==0:\n",
    "                dict_strength_connectivity[y][node]=0\n",
    "            else:\n",
    "                if list(dict_new[y]).count(node)==0:\n",
    "                    dict_strength_connectivity[y][node]=0\n",
    "                else:\n",
    "                    dict_strength_connectivity[y][node]=dict_new[y][node]/ptot[node]\n",
    "    \n",
    "    \n",
    "    div=dict()\n",
    "    for node in list_of_nodes:\n",
    "        div[node]=0\n",
    "    for node in list_of_nodes:\n",
    "        s=0\n",
    "        for y in temps:\n",
    "            p=dict_strength_connectivity[y][node]\n",
    "            if p!=0:\n",
    "                s=s-p*math.log(p)\n",
    "        div[node]=math.exp(s)\n",
    "        \n",
    "    divT=sum(list(div.values()))\n",
    "    teta=1-(divT-len(list_of_nodes))/(len(list_of_nodes)*(len(temps)-1))\n",
    "    \n",
    "    dict_alpha_taxonomic_coccurrence2[index_gsa]=round(divT,2)    \n",
    "    dict_teta_taxonomic_coccurrence[index_gsa]=round(teta,2)   \n",
    "\n",
    "\n",
    "#print(dict_alpha_taxonomic_coccurrence2)\n",
    "print('Temporal variation species: ',dict_teta_taxonomic_coccurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6967024",
   "metadata": {},
   "source": [
    "# Combination Co-occurence & Phylogenetic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_alpha_phylogenetic_coccurrence=dict()\n",
    "for index_gsa in l_gsa:\n",
    "    \n",
    "    dict_new=dict()\n",
    "    for y in temps:\n",
    "        dict_new[y]=dict()    #dict(year: {specie: measure})\n",
    "        \n",
    "    for y in temps:\n",
    "        \n",
    "        gpa9=gsa[index_gsa][y]\n",
    "    \n",
    "        lgpa9=[] # list of species of each line which have a non null abd value [[specices with not null abd of line1], [spc ... line2] ...]\n",
    "        for i in gpa9:\n",
    "            l=[]\n",
    "            for j in gpa9[i]:\n",
    "                if(gpa9[i][j]!=0):\n",
    "                    l.append(j)\n",
    "            lgpa9.append(l)\n",
    "        #print('lgpa9=',lgpa9)\n",
    "    \n",
    "        lt=[] #lt=[construction of edges based on each line]\n",
    "        for l in lgpa9:\n",
    "            for i in range(0,len(l)-1):\n",
    "                for j in range(i+1,len(l)):\n",
    "                    lt.append((l[i],l[j]))\n",
    "        #print('lt=',lt)\n",
    "        #print(len(lt))\n",
    "    \n",
    "    \n",
    "        lt2=list(set(lt)) #lt2=[eliminate the double edges to obtain list with all the unique edges in the network]\n",
    "        #print('lt2=',lt2)\n",
    "        #print(len(lt2))\n",
    "        for i in range(0,len(lt2)-1):\n",
    "            for j in range(i+1,len(lt2)):\n",
    "                if (lt2[i][0]==lt2[j][1]) and (lt2[i][1]==lt2[j][0]):\n",
    "                    lt2.remove(lt2[j][0],lt2[j][1])\n",
    "    \n",
    "    \n",
    "        ################ Building Graph based on the nbr of selections #################\n",
    "        dict_lt=dict() #dict_lt{edge: number of selection}\n",
    "        for i in lt2:\n",
    "            dict_lt[i]=lt.count(i)\n",
    "    \n",
    "    \n",
    "        ltVF=[]\n",
    "        for i in dict_lt:\n",
    "            ltVF.append((i[0],i[1],dict_lt[i]))\n",
    "        #print('lt2=',lt2)\n",
    "    \n",
    "        g=nx.Graph()\n",
    "        g.add_weighted_edges_from(ltVF)\n",
    "        #size[gsa][y]=len(g)\n",
    "        #print(len(g))\n",
    "        #nbr_edges[gsa][y]=len(list(g.edges()))\n",
    "        ################################################################################\n",
    "    \n",
    "    \n",
    "        ################ Building Graph based on phylogenetic matrix ###################\n",
    "        dict_lt_phylogenetic=dict() #dict_lt_phylogenetic{edge: phylogenetic distance}\n",
    "        for i in lt2:\n",
    "            if (l_phylogenetic.count(i[0])==0) or (l_phylogenetic.count(i[1])==0):\n",
    "                dict_lt_phylogenetic[i]=0\n",
    "            else:\n",
    "                dict_lt_phylogenetic[i]=matrice_phylogenetic[indices_phylogenetic[i[0]]][indices_phylogenetic[i[1]]]\n",
    "    \n",
    "    \n",
    "        nbr_removed=0\n",
    "        ltVF=[]\n",
    "        for i in dict_lt_phylogenetic:\n",
    "            if (dict_lt_phylogenetic[i]==0):\n",
    "                nbr_removed=nbr_removed+1\n",
    "            else:\n",
    "                ltVF.append((i[0],i[1],dict_lt_phylogenetic[i]))\n",
    "        #print('lt2=',lt2)\n",
    "    \n",
    "        g_phylogenetic=nx.Graph()\n",
    "        g_phylogenetic.add_weighted_edges_from(ltVF)\n",
    "        ################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        dict_specie_abd=dict()\n",
    "        for sp in l_species:\n",
    "            dict_specie_abd[sp]=0\n",
    "                \n",
    "        # Data_gsa[i] : list of species of each line = gpa9\n",
    "        Data_gsa=gpa9\n",
    "            \n",
    "        for i in Data_gsa:\n",
    "            for j in Data_gsa[i]:\n",
    "                dict_specie_abd[j]=dict_specie_abd[j]+float(Data_gsa[i][j])\n",
    "        \n",
    "        edges=list(g.edges(data=True))\n",
    "        for e in edges:\n",
    "            #x=(2*e[2]['weight'])/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]])\n",
    "            #if x<=1:\n",
    "            #    print('hana')\n",
    "            e[2]['weight']=1-((abs(dict_specie_abd[e[0]]-dict_specie_abd[e[1]]))/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]]))\n",
    "            \n",
    "        #print(list(g.edges(data=True)))\n",
    "        \n",
    "        \n",
    "        dict_strength=dict()\n",
    "        l_strg=list()\n",
    "        for node in g:\n",
    "            s=0\n",
    "            l=g.edges(node, data=True)\n",
    "            for e in l:\n",
    "                s=s+e[2]['weight']\n",
    "            dict_strength[node]=s\n",
    "            \n",
    "        dict_strength_phylogenetic=dict()\n",
    "        for node in g_phylogenetic:\n",
    "            s=0\n",
    "            l=g_phylogenetic.edges(node, data=True)\n",
    "            for e in l:\n",
    "                s=s+e[2]['weight']\n",
    "            dict_strength_phylogenetic[node]=s\n",
    "         \n",
    "        for node in g_phylogenetic:\n",
    "            x=dict_strength_phylogenetic[node]*dict_strength[node]\n",
    "            dict_strength_phylogenetic[node]=x\n",
    "            l_strg.append(x)\n",
    "            \n",
    "        for node in g_phylogenetic:   \n",
    "            dict_strength_phylogenetic[node]=dict_strength_phylogenetic[node]/max(l_strg)\n",
    "        \n",
    "            \n",
    "        \n",
    "        dict_degree=nx.degree_centrality(g_phylogenetic)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in dict_strength_phylogenetic:\n",
    "            #dict_new[i]=dict_strength[i]/dict_degree[i]\n",
    "            dict_new[y][i]=math.sqrt(dict_strength_phylogenetic[i]*dict_strength_phylogenetic[i]+dict_degree[i]*dict_degree[i])\n",
    "            #if(dict_new[i]>=1):\n",
    "            #    print('hana')\n",
    "        \n",
    "        #print('dict_new',dict_new)\n",
    "        ptot=sum(list(dict_new[y].values()))\n",
    "        #print(ptot)\n",
    "            \n",
    "        for i in dict_new[y]:\n",
    "            if ptot==0:\n",
    "                dict_new[y][i]=0\n",
    "            else:\n",
    "                dict_new[y][i]=dict_new[y][i]/ptot\n",
    "                \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    d_species_abd_time=dict()  # dict(species:{t1: p1, t2: p2 ......})\n",
    "    for i in l_species:\n",
    "        d_species_abd_time[i]=dict()\n",
    "        for j in temps:\n",
    "            d_species_abd_time[i][j]=0\n",
    "            \n",
    "    \n",
    "    for i in l_species:\n",
    "        for y in temps:\n",
    "            Data_gsa=dict_new[y]\n",
    "            if list(Data_gsa).count(i)!=0:\n",
    "                d_species_abd_time[i][y]=Data_gsa[i]\n",
    "                \n",
    "    #print()\n",
    "    #print()\n",
    "    #print('d_species_abd_time', d_species_abd_time)\n",
    "\n",
    "\n",
    "\n",
    "    d_species_abdlist=dict()   #dict(species: {abd: nbr of years, })\n",
    "    for i in d_species_abd_time:\n",
    "        d_species_abdlist[i]=dict()\n",
    "    #print()\n",
    "    #print()\n",
    "    #print(d_species_abdlist)\n",
    "    for i in d_species_abd_time:\n",
    "        #print(d_species_abd_time)\n",
    "        l=list(d_species_abd_time[i].values())\n",
    "        #print()\n",
    "        #print()\n",
    "        #print(l)\n",
    "        list_abd=list(set(l))\n",
    "    \n",
    "        for j in list_abd:\n",
    "            d_species_abdlist[i][j]=l.count(j)\n",
    "\n",
    "\n",
    "    s=0\n",
    "    for i in d_species_abdlist:\n",
    "        for abd in d_species_abdlist[i]:\n",
    "            if abd!=0:\n",
    "                s=s-((d_species_abdlist[i][abd]/deltaT)*abd)*math.log(abd)\n",
    "    #print(s)\n",
    "    dict_alpha_phylogenetic_coccurrence[index_gsa]=round(math.exp(s),2)\n",
    "\n",
    "for i in dict_alpha_phylogenetic_coccurrence:\n",
    "    dict_alpha_phylogenetic_coccurrence[i]=(dict_alpha_phylogenetic_coccurrence[i]-1)/(124-1)\n",
    "print('Temporal Diversity: ',dict_alpha_phylogenetic_coccurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87507225",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_alpha_phylogenetic_coccurrence2=dict()\n",
    "dict_teta_phylogenetic_coccurrence=dict()\n",
    "\n",
    "for index_gsa in l_gsa:\n",
    "    \n",
    "    dict_new=dict()\n",
    "    for y in temps:\n",
    "        dict_new[y]=dict()    #dict(year: {specie: measure})\n",
    "        \n",
    "    print()\n",
    "    for y in temps:\n",
    "        \n",
    "        gpa9=gsa[index_gsa][y]\n",
    "    \n",
    "        lgpa9=[] # list of species of each line which have a non null abd value [[specices with not null abd of line1], [spc ... line2] ...]\n",
    "        for i in gpa9:\n",
    "            l=[]\n",
    "            for j in gpa9[i]:\n",
    "                if(gpa9[i][j]!=0):\n",
    "                    l.append(j)\n",
    "            lgpa9.append(l)\n",
    "        #print('lgpa9=',lgpa9)\n",
    "    \n",
    "        lt=[] #lt=[construction of edges based on each line]\n",
    "        for l in lgpa9:\n",
    "            for i in range(0,len(l)-1):\n",
    "                for j in range(i+1,len(l)):\n",
    "                    lt.append((l[i],l[j]))\n",
    "        #print('lt=',lt)\n",
    "        #print(len(lt))\n",
    "    \n",
    "    \n",
    "        lt2=list(set(lt)) #lt2=[eliminate the double edges to obtain list with all the unique edges in the network]\n",
    "        #print('lt2=',lt2)\n",
    "        #print(len(lt2))\n",
    "        for i in range(0,len(lt2)-1):\n",
    "            for j in range(i+1,len(lt2)):\n",
    "                if (lt2[i][0]==lt2[j][1]) and (lt2[i][1]==lt2[j][0]):\n",
    "                    lt2.remove(lt2[j][0],lt2[j][1])\n",
    "    \n",
    "    \n",
    "        ################ Building Graph based on the nbr of selections #################\n",
    "        dict_lt=dict() #dict_lt{edge: number of selection}\n",
    "        for i in lt2:\n",
    "            dict_lt[i]=lt.count(i)\n",
    "    \n",
    "    \n",
    "        ltVF=[]\n",
    "        for i in dict_lt:\n",
    "            ltVF.append((i[0],i[1],dict_lt[i]))\n",
    "        #print('lt2=',lt2)\n",
    "    \n",
    "        g=nx.Graph()\n",
    "        g.add_weighted_edges_from(ltVF)\n",
    "        #size[gsa][y]=len(g)\n",
    "        #print(len(g))\n",
    "        #nbr_edges[gsa][y]=len(list(g.edges()))\n",
    "        ################################################################################\n",
    "    \n",
    "    \n",
    "        ################ Building Graph based on phylogenetic matrix ###################\n",
    "        dict_lt_phylogenetic=dict() #dict_lt_phylogenetic{edge: phylogenetic distance}\n",
    "        for i in lt2:\n",
    "            if (l_phylogenetic.count(i[0])==0) or (l_phylogenetic.count(i[1])==0):\n",
    "                dict_lt_phylogenetic[i]=0\n",
    "            else:\n",
    "                dict_lt_phylogenetic[i]=matrice_phylogenetic[indices_phylogenetic[i[0]]][indices_phylogenetic[i[1]]]\n",
    "    \n",
    "    \n",
    "        nbr_removed=0\n",
    "        ltVF=[]\n",
    "        for i in dict_lt_phylogenetic:\n",
    "            if (dict_lt_phylogenetic[i]==0):\n",
    "                nbr_removed=nbr_removed+1\n",
    "            else:\n",
    "                ltVF.append((i[0],i[1],dict_lt_phylogenetic[i]))\n",
    "        #print('lt2=',lt2)\n",
    "    \n",
    "        g_phylogenetic=nx.Graph()\n",
    "        g_phylogenetic.add_weighted_edges_from(ltVF)\n",
    "        ################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        dict_specie_abd=dict()\n",
    "        for sp in l_species:\n",
    "            dict_specie_abd[sp]=0\n",
    "                \n",
    "        # Data_gsa[i] : list of species of each line = gpa9\n",
    "        Data_gsa=gpa9\n",
    "            \n",
    "        for i in Data_gsa:\n",
    "            for j in Data_gsa[i]:\n",
    "                dict_specie_abd[j]=dict_specie_abd[j]+float(Data_gsa[i][j])\n",
    "        \n",
    "        edges=list(g.edges(data=True))\n",
    "        for e in edges:\n",
    "            #x=(2*e[2]['weight'])/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]])\n",
    "            #if x<=1:\n",
    "            #    print('hana')\n",
    "            e[2]['weight']=1-((abs(dict_specie_abd[e[0]]-dict_specie_abd[e[1]]))/(dict_specie_abd[e[0]]+dict_specie_abd[e[1]]))\n",
    "            \n",
    "        #print(list(g.edges(data=True)))\n",
    "        \n",
    "        \n",
    "        dict_strength=dict()\n",
    "        l_strg=list()\n",
    "        for node in g:\n",
    "            s=0\n",
    "            l=g.edges(node, data=True)\n",
    "            for e in l:\n",
    "                s=s+e[2]['weight']\n",
    "            dict_strength[node]=s\n",
    "            \n",
    "        dict_strength_phylogenetic=dict()\n",
    "        for node in g_phylogenetic:\n",
    "            s=0\n",
    "            l=g_phylogenetic.edges(node, data=True)\n",
    "            for e in l:\n",
    "                s=s+e[2]['weight']\n",
    "            dict_strength_phylogenetic[node]=s\n",
    "         \n",
    "        for node in g_phylogenetic:\n",
    "            x=dict_strength_phylogenetic[node]*dict_strength[node]\n",
    "            dict_strength_phylogenetic[node]=x\n",
    "            l_strg.append(x)\n",
    "            \n",
    "        for node in g_phylogenetic:   \n",
    "            dict_strength_phylogenetic[node]=dict_strength_phylogenetic[node]/max(l_strg)\n",
    "        \n",
    "            \n",
    "        \n",
    "        dict_degree=nx.degree_centrality(g_phylogenetic)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in dict_strength_phylogenetic:\n",
    "            #dict_new[i]=dict_strength[i]/dict_degree[i]\n",
    "            dict_new[y][i]=math.sqrt(dict_strength_phylogenetic[i]*dict_strength_phylogenetic[i]+dict_degree[i]*dict_degree[i])\n",
    "            #if(dict_new[i]>=1):\n",
    "            #    print('hana')\n",
    "        \n",
    "        #print('dict_new',dict_new)\n",
    "        #ptot=sum(list(dict_new[y].values()))\n",
    "        #print(ptot)\n",
    "        \n",
    "    list_of_nodes=set()\n",
    "    for y in temps:\n",
    "        for node in dict_new[y]:\n",
    "            list_of_nodes.add(node)\n",
    "    list_of_nodes=list(list_of_nodes)\n",
    "    \n",
    "    \n",
    "    ptot=dict()\n",
    "    for node in list_of_nodes:\n",
    "        ptot[node]=0\n",
    "    for node in list_of_nodes:\n",
    "        for y in temps:\n",
    "            if list(dict_new[y]).count(node)!=0:\n",
    "                ptot[node]=ptot[node]+dict_new[y][node]\n",
    "    \n",
    "    \n",
    "    dict_strength_connectivity=dict()\n",
    "    for y in temps:\n",
    "        dict_strength_connectivity[y]=dict()\n",
    "        for node in list_of_nodes:\n",
    "            dict_strength_connectivity[y][node]=0                    \n",
    "    for node in list_of_nodes:\n",
    "        for y in temps:\n",
    "            if ptot[node]==0:\n",
    "                dict_strength_connectivity[y][node]=0\n",
    "            else:\n",
    "                if list(dict_new[y]).count(node)==0:\n",
    "                    dict_strength_connectivity[y][node]=0\n",
    "                else:\n",
    "                    dict_strength_connectivity[y][node]=dict_new[y][node]/ptot[node]\n",
    "    \n",
    "    \n",
    "    div=dict()\n",
    "    for node in list_of_nodes:\n",
    "        div[node]=0\n",
    "    for node in list_of_nodes:\n",
    "        s=0\n",
    "        for y in temps:\n",
    "            p=dict_strength_connectivity[y][node]\n",
    "            if p!=0:\n",
    "                s=s-p*math.log(p)\n",
    "        div[node]=math.exp(s)\n",
    "    #print(len(list_of_nodes))    \n",
    "    divT=sum(list(div.values()))\n",
    "    teta=1-(divT-124)/(124*(len(temps)-1))\n",
    "    \n",
    "    dict_alpha_phylogenetic_coccurrence2[index_gsa]=round(divT,2)    \n",
    "    dict_teta_phylogenetic_coccurrence[index_gsa]=round(teta,2)   \n",
    "\n",
    "\n",
    "#print(dict_alpha_phylogenetic_coccurrence2)\n",
    "print('Temporal variation species: ',dict_teta_phylogenetic_coccurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f3b95",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e59af2",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7803a4",
   "metadata": {},
   "source": [
    "# **************** Save the results *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************************** Co-occurence *************************************************\n",
    "l=[]\n",
    "\n",
    "#l.append(['','Tdiv','Tvar'])\n",
    "for gs in l_gsa:\n",
    "    #print(dict_alpha[gs])\n",
    "    l.append(['GSA'+' '+str(gs)]+[dict_alpha[gs],dict_teta[gs]])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(np.array(l),\n",
    "                   columns=['GSAs','Temporal diversity','Temporal variation of species'])\n",
    "\n",
    "print (df.head())\n",
    "name='OutputFiles/T_div_Tvar_AllGSAs.xlsx'\n",
    "df.to_excel(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae37098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "data=pd.read_excel(\"OutputFiles/T_div_Tvar_AllGSAs.xlsx\", index_col=None,na_values=['NA'])\n",
    "\n",
    "print('************** Co-occurrence *****************')\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "\n",
    "fig = px.scatter(data, x=\"Temporal diversity\", y=\"Temporal variation of species\", text=\"GSAs\", log_x=True, size_max=100, color=\"GSAs\")\n",
    "fig.update_traces(textposition='top center')\n",
    "#fig.update_layout(title_text='Co-occurrence', title_x=0.5)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
